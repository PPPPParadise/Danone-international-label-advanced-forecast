{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os, gc\n",
    "from scipy import stats\n",
    "from os import listdir \n",
    "from os.path import isfile, join\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import src.kpi.kpis_computation as kpis\n",
    "import matplotlib.pyplot as plt\n",
    "import lunarcalendar \n",
    "%matplotlib inline\n",
    "# import KPI_formalization_combined as kpi_stats\n",
    "import src.kpi.KPI_formalization as kpi_stats\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 99 \n",
    "from typing import List\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "import src.forecaster.utilitaires as util\n",
    "import src.forecaster.modelil as mod \n",
    "from src.forecaster.features import *\n",
    "from collections import namedtuple\n",
    "from src.forecaster.MLDCModel import MLDCModel\n",
    "from src.forecaster.features import *\n",
    "from src.forecaster.model import Model\n",
    "ModelConfig = namedtuple(\"ModelConfig\", \"model_name model_params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "#path \n",
    "config[\"project_folder_path\"] = \"..\"\n",
    "config[\"data_folder_path\"] = \"data\"\n",
    "config[\"temp_folder_path\"] = \"temp\"\n",
    "config[\"result_folder_path\"] = \"sub_brand\"\n",
    "\n",
    "#input files\n",
    "config[\"input_raw_master\"] = \"raw_master_il_20200210_Update_0602_subbrand_catupdate_v2.csv\"\n",
    "config[\"input_category_forecast\"] = \"IL_feature_table_all_0610_cat_fsct.csv\"\n",
    "\n",
    "#temp files\n",
    "config[\"feature_import_first_run\"] = \"feature_importance_df_sets_0615_RF.csv\"\n",
    "config[\"version_name\"] = '0615_with_Q2_40_feature'\n",
    "\n",
    "# Parameter configuration\n",
    "config[\"train_start\"] = 201601\n",
    "config[\"train_end\"] = 201912\n",
    "config[\"backtest_start\"] = 201801\n",
    "config[\"backtest_end\"] = 201912\n",
    "config[\"FirstRun\"] = False \n",
    "config['horizon'] = 12\n",
    "\n",
    "# Columns configuration\n",
    "config[\"features_int\"] = [\n",
    "    \"date_when_predicting\", \n",
    "    \"label\",\n",
    "    \"date_to_predict\",\n",
    "    \"target\",\n",
    "    \"country\", \n",
    "    \"brand\",\n",
    "    \"horizon\",\n",
    "    \"country_brand_channel\",\n",
    "    \"country_brand\",\n",
    "    \"sub_brand\",\n",
    "    \"tier\"\n",
    "]\n",
    "\n",
    "config[\"features_cat_col\"] = [\n",
    "    '0to6_month_population_mean_3M',\n",
    "    '6to12_month_population_mean_3M',\n",
    "    '12to36_month_population_mean_3M',\n",
    "    '0to6_month_population_mean_6M',\n",
    "    '6to12_month_population_mean_6M',\n",
    "    '12to36_month_population_mean_6M',\n",
    "    '0to6_month_population_mean_9M',\n",
    "    '6to12_month_population_mean_9M',\n",
    "    '12to36_month_population_mean_9M',\n",
    "    '0to6_month_population_mean_12M',\n",
    "    '6to12_month_population_mean_12M',\n",
    "    '12to36_month_population_mean_12M'\n",
    "]   \n",
    "\n",
    "config[\"features_cat_fsct_col\"] = [\n",
    "    'upre_fsct',\n",
    "     'spre_fsct',\n",
    "    'mainstream_fsct',\n",
    "    'upre_mean_3M_fsct',\n",
    "    'spre_mean_3M_fsct',\n",
    "    'mainstream_mean_3M_fsct'\n",
    "] \n",
    "\n",
    "config[\"feature_test\"] = [\n",
    "    'label', 'country', 'brand', 'tier', 'country_brand', 'sub_brand',\n",
    "    'date_when_predicting', 'date_to_predict', 'target', \n",
    "    'upre_sales3','upre_sales2', 'upre_sales1',\n",
    "    'spre_sales3', 'spre_sales2','spre_sales1', \n",
    "    'mainstream_sales3', 'mainstream_sales2','mainstream_sales1', \n",
    "    'month', 'sin_month', 'cos_month', \n",
    "    'ANZ_APT','ANZ_KC', 'DE_APT', 'NL_NC', 'UK_APT', 'UK_C&G',\n",
    "    'il', 'C&G', 'GD','GT', 'PF', 'PN', 'horizon', \n",
    "    'sub_brand_offtake_il_mean_3M','sub_brand_sellin_il_mean_3M', \n",
    "    'sub_brand_offtake_di_mean_3M','price_tier_cat_mean_3M', \n",
    "    'sub_brand_offtake_il_mean_6M','sub_brand_sellin_il_mean_6M',\n",
    "    'sub_brand_offtake_di_mean_6M','price_tier_cat_mean_6M', \n",
    "    'sub_brand_offtake_il_mean_9M','sub_brand_sellin_il_mean_9M', \n",
    "    'sub_brand_offtake_di_mean_9M','price_tier_cat_mean_9M', \n",
    "    'sub_brand_offtake_il_mean_12M','sub_brand_sellin_il_mean_12M',\n",
    "    'sub_brand_offtake_di_mean_12M','price_tier_cat_mean_12M', \n",
    "    '0to6_month_population_mean_3M','6to12_month_population_mean_3M', \n",
    "    '12to36_month_population_mean_3M','0to6_month_population_mean_6M', \n",
    "    '6to12_month_population_mean_6M','12to36_month_population_mean_6M', \n",
    "    '0to6_month_population_mean_9M', '6to12_month_population_mean_9M', \n",
    "    '12to36_month_population_mean_9M','0to6_month_population_mean_12M', \n",
    "    '6to12_month_population_mean_12M','12to36_month_population_mean_12M' \n",
    "    'upre_fsct', 'spre_fsct','mainstream_fsct', 'upre_mean_3M_fsct', \n",
    "    'spre_mean_3M_fsct','mainstream_mean_3M_fsct'\n",
    "] \n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "ModelConfig = namedtuple(\"ModelConfig\", \"model_name model_params\")\n",
    "\n",
    "config[\"model_config_XGBRegressor\"] =  ModelConfig(\n",
    "    model_name=\"XGBRegressor\",\n",
    "    model_params={\n",
    "        'max_depth': 8,\n",
    "        'gamma': 0.02,\n",
    "        'subsample': 0.3,\n",
    "        'n_estimators': 60,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_jobs': 12,\n",
    "        'verbosity': 2})\n",
    "\n",
    "config[\"model_config_RandomForestRegressor\"] = ModelConfig(\n",
    "        model_name=\"RandomForestRegressor\",\n",
    "        model_params={\n",
    "            'max_depth': 8,\n",
    "            'n_estimators': 80,\n",
    "            'max_features':40,\n",
    "            'n_jobs': 12}) \n",
    "\n",
    "config[\"model_config_ExtraTreesRegressor\"] = ModelConfig(\n",
    "        model_name=\"ExtraTreesRegressor\",\n",
    "        model_params={\n",
    "            'max_depth': 8,\n",
    "            'n_estimators': 60,\n",
    "            'max_features':50,\n",
    "            'n_jobs': 12}) \n",
    "\n",
    "config[\"model_config_AdaBoostRegressor\"] = ModelConfig(\n",
    "        model_name=\"AdaBoostRegressor\",\n",
    "        model_params={\n",
    "            'n_estimators': 80,\n",
    "            'learning_rate': 0.2,\n",
    "            'loss':'square'}) \n",
    "\n",
    "config[\"model_config_GradientBoostingRegressor\"] = ModelConfig(\n",
    "        model_name=\"GradientBoostingRegressor\",\n",
    "        model_params={ \n",
    "            'subsample': 0.3,\n",
    "            'n_estimators': 80,\n",
    "            'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_50_feature(config):\n",
    "    \n",
    "    feature_path = config[\"project_folder_path\"] + '/' +\\\n",
    "                   config[\"temp_folder_path\"] + '/' +\\\n",
    "                   config[\"feature_import_first_run\"]\n",
    "    feature_importance_df_sets = pd.read_csv(feature_path, index_col = 0)\n",
    "    \n",
    "    return feature_importance_df_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_raw_master(config):\n",
    "    \n",
    "    raw_master_path = config[\"project_folder_path\"] + '/' +\\\n",
    "                      config[\"data_folder_path\"] + '/' +\\\n",
    "                      config[\"input_raw_master\"]\n",
    "    \n",
    "    df =  pd.read_csv(raw_master_path)\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['country_brand'] = df.apply( lambda x:x['country']+'_'+x['brand'], axis = 1)\n",
    "    df['sub_brand'] = df.apply( lambda x:x['country']+'_'+x['brand']+'_'+x['tier'], axis = 1)\n",
    "    df['calendar_yearmonth'] = pd.to_datetime(df['date']).dt.year.astype(\n",
    "                str) + pd.to_datetime(df['date']).dt.month.astype(str).str.zfill(2)\n",
    "    df['calendar_yearmonth'] = df['calendar_yearmonth'].astype(int)\n",
    "    df['sub_brand_offtake_il'] = df.groupby(['date','sub_brand'])['offtake_il'].transform(sum)\n",
    "    df['sub_brand_offtake_di'] = df.groupby(['date','sub_brand'])['offtake_di'].transform(sum)\n",
    "    df['sub_brand_offtake_eib'] = df.groupby(['date','sub_brand'])['offtake_eib'].transform(sum)\n",
    "    df['sub_brand_sellin_eib'] = df.groupby(['date','sub_brand'])['sellin_eib'].transform(sum)\n",
    "    df['sub_brand_sellin_il'] = df.groupby(['date','sub_brand'])['sellin_il'].transform(sum)\n",
    "    df['sub_brand_sellin_di'] = df.groupby(['date','sub_brand'])['sellin_di'].transform(sum)\n",
    "    df['sub_brand_retailer_inv'] = df.groupby(['date','sub_brand'])['retailer_inv'].transform(sum)\n",
    "    df['sub_brand_sellout'] = df.groupby(['date','sub_brand'])['sellout'].transform(sum)\n",
    "    df['sub_brand_sp_inv'] = df.groupby(['date','sub_brand'])['sp_inv'].transform(sum)\n",
    "    df['sub_brand_price'] = df.groupby(['date','sub_brand'])['price'].transform(np.mean)\n",
    "    df['sub_brand_price_krmb_per_ton_il'] = df.groupby(['date','sub_brand'])['price_krmb_per_ton_il'].transform(np.mean)\n",
    "    df['sub_brand_value_krmb_il'] = df.groupby(['date','sub_brand'])['value_krmb_il'].transform(np.mean)\n",
    "    df['volume_ton_il'] = df.groupby(['date','sub_brand'])['volume_ton_il'].transform(np.mean)\n",
    "    df['uprep'] = df.groupby(['date','sub_brand'])['uprep'].transform(np.mean)\n",
    "    df['upre'] = df.groupby(['date','sub_brand'])['upre'].transform(np.mean)\n",
    "    df['spre'] = df.groupby(['date','country_brand'])['spre'].transform(np.mean)\n",
    "    df['mainstream'] = df.groupby(['date','country_brand'])['mainstream'].transform(np.mean)\n",
    "    df['anp'] = df.groupby(['date','sub_brand'])['anp'].transform(np.mean)\n",
    "    df['tp'] = df.groupby(['date','sub_brand'])['tp'].transform(np.mean)\n",
    "    df['rebate'] = df.groupby(['date','country_brand'])['rebate'].transform(np.mean)\n",
    "    df['tp_rebate'] = df.groupby(['date','country_brand'])['tp_rebate'].transform(np.mean)\n",
    "    df['sc_anp'] = df.groupby(['date','country_brand'])['sc_anp'].transform(np.mean)\n",
    "    df['sc_ts'] = df.groupby(['date','country_brand'])['sc_ts'].transform(np.mean)\n",
    "\n",
    "    features_search_index = [c for c in df.keys() if ('_si' in c)] \n",
    "    for col in features_search_index:\n",
    "        df[col] = df.groupby(['date','sub_brand'])[col].transform(np.mean)\n",
    "\n",
    "    brands = [c for c in df.columns if 'sub_brand' in c]  \n",
    "\n",
    "    sel_col = ['country', 'brand', 'date','tier',\n",
    "               'isa', 'osa', \n",
    "               'total_vol',\n",
    "               'if_vol', 'fo_vol', 'gum_vol', 'cl_vol', 'il_vol',\n",
    "               '0to6_month_population', '6to12_month_population',\n",
    "               '12to36_month_population', 'uprep', 'upre', 'spre',\n",
    "               'mainstream', 'country_brand','sub_brand',\n",
    "               'calendar_yearmonth', 'sub_brand_offtake_il', 'sub_brand_offtake_di',\n",
    "               'sub_brand_offtake_eib', 'sub_brand_sellin_eib', 'sub_brand_retailer_inv',\n",
    "               'sub_brand_sellout', 'sub_brand_sp_inv', 'sub_brand_sellin_di', 'sub_brand_price',\n",
    "               'sub_brand_price_krmb_per_ton_il', 'sub_brand_value_krmb_il','sub_brand_sellin_il',\n",
    "                'anp', 'tp', 'rebate', 'tp_rebate', 'sc_anp', 'sc_ts'] +\\\n",
    "                features_search_index  \n",
    "\n",
    "    df_ = df.drop_duplicates(['date', 'sub_brand'])\n",
    "    df_ = df_[sel_col]\n",
    "    price_tier_dict = {'ANZ_APT_PF':'upre',\n",
    "                       'ANZ_APT_PN':'spre',              \n",
    "                       'ANZ_KC_GD':'mainstream', \n",
    "                       'ANZ_KC_GT':'spre',\n",
    "                       'DE_APT_PF':'upre', \n",
    "                       'DE_APT_PN':'mainstream',\n",
    "                       'NL_NC_PN':'mainstream', \n",
    "                       'UK_APT_PF':'upre', \n",
    "                       'UK_APT_PN':'mainstream',\n",
    "                       'UK_C&G_C&G':'mainstream'} \n",
    "\n",
    "    df_['price_tier_cat']= df_.apply(lambda x:x[price_tier_dict[x.sub_brand]], axis = 1)  \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observed_sales(res: pd.DataFrame, \n",
    "                       label: str,brand: str,\n",
    "                       year: int, month: int, date_name: str, value_col: str) -> float:\n",
    "    \"\"\" Function to get the ratio of sales of one month compared to its surrounding months\n",
    "\n",
    "    :param value_col:\n",
    "    :param date_name:\n",
    "    :param res: dataframe containing the sales\n",
    "    :param label: label for which we want to compute the ratio\n",
    "    :param year: year for which we want to compute the ratio\n",
    "    :param month: month for which we want to compute the ratio\n",
    "    :param date_name: name of the date column\n",
    "    :return: the ratio\n",
    "    \"\"\"\n",
    "\n",
    "    temp = res.copy()\n",
    "    \n",
    "    ob = temp[(temp[date_name] == (year * 100 + month - 1)) & (temp['sub_brand'] == brand)][value_col].sum() + \\\n",
    "         temp[(temp[date_name] == (year * 100 + month + 1)) & (temp['sub_brand'] == brand)][value_col].sum()\n",
    "    ac = temp[(temp[date_name] == (year * 100 + month)) & (temp['sub_brand'] == brand)][value_col].sum() \n",
    "    return ac / ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def apply_forecast_correction(sales, forecast, forecast_filtered, label,brand, year, month, thrsh=0.05):\n",
    "    \"\"\" This function is used to apply the forecast correction depending upon the rule described in the documentation\n",
    "\n",
    "    :param sales: raw_master containing the original historical sales data\n",
    "    :param forecast: dataframe containing the full forecast\n",
    "    :param forecast_filtered: dataframe containing forecasts corresponding to the chosen label\n",
    "    :param label: chosen label\n",
    "    :param year: int\n",
    "    :param month: int\n",
    "    :param thrsh: threshold below which the correction is not applied\n",
    "    :return: dataframe containing the corrected forecast\n",
    "    \"\"\"\n",
    "\n",
    "    def get_cny_month(cny_year):\n",
    "        cny_date = lunarcalendar.festival.ChineseNewYear(cny_year)\n",
    "        cny_month = cny_date.month\n",
    "\n",
    "        if cny_date.day > 24:\n",
    "            cny_month += 1\n",
    "\n",
    "        return cny_month\n",
    "\n",
    "    year_minus_1 = year - 1\n",
    "    month_y1 = month\n",
    "    year_minus_2 = year - 2\n",
    "    month_y2 = month\n",
    "    year_minus_3 = year - 3\n",
    "    month_y3 = month\n",
    "    year_minus_4 = year - 4\n",
    "    month_y4 = month\n",
    "\n",
    "    if month == 'CNY':\n",
    "        month = get_cny_month(year)\n",
    "        month_y1 = get_cny_month(year_minus_1)\n",
    "        month_y2 = get_cny_month(year_minus_2)\n",
    "        \n",
    "\n",
    "    correction_condition = (int(str(year * 100 + month)) != forecast.date_to_predict.max()) & \\\n",
    "                           (int(str(year * 100 + month)) != forecast.date_to_predict.min())\n",
    "    \n",
    "    # When first month of forecast need adjust\n",
    "    if int(str(year * 100 + month)) == forecast.date_to_predict.min():\n",
    "        actual_ = sales[sales.calendar_yearmonth == int(str(year * 100 + month))-1]\n",
    "        actual_['label'] = label\n",
    "        actual_['brand'] = brand\n",
    "        actual_['horizon'] = 0\n",
    "        actual_.rename(columns = {'calendar_yearmonth':'date_to_predict',\n",
    "                                  'offtake':'prediction'}, inplace = True)\n",
    "        actual_ = actual_[forecast.columns]\n",
    "        forecast = pd.concat([actual_, forecast])\n",
    "        correction_condition = True\n",
    "\n",
    "    if correction_condition:\n",
    "        if month==11:\n",
    "            tar = get_observed_sales(sales, label,brand,year_minus_4, month_y4, \n",
    "                                     'calendar_yearmonth', 'offtake')*0.1 + \\\n",
    "                  get_observed_sales(sales, label,brand, year_minus_3, month_y3, \n",
    "                                     'calendar_yearmonth', 'offtake')*0.2 + \\\n",
    "                  get_observed_sales(sales, label,brand, year_minus_2, month_y2, \n",
    "                                     'calendar_yearmonth', 'offtake')*0.3 + \\\n",
    "                  get_observed_sales(sales, label,brand, year_minus_1, month_y1, \n",
    "                                     'calendar_yearmonth', 'offtake')*0.4\n",
    "        else:\n",
    "            tar = get_observed_sales(sales, label,brand,year_minus_2, month_y2,\n",
    "                                     'calendar_yearmonth', 'offtake')*0.2 + \\\n",
    "                  get_observed_sales(sales, label,brand, year_minus_1, month_y1,\n",
    "                                     'calendar_yearmonth', 'offtake')*0.8\n",
    "        acoc = get_observed_sales(forecast, \n",
    "                                  label,brand,\n",
    "                                  year, month, 'date_to_predict', 'prediction') \n",
    "\n",
    "        mf = tar / acoc\n",
    "\n",
    "\n",
    "        if np.abs(tar - acoc) > thrsh:\n",
    "\n",
    "            forecast_filtered.loc[(forecast_filtered.date_to_predict == year * 100 + month)&(forecast_filtered.sub_brand==brand), 'prediction'] *= mf\n",
    "\n",
    "    return forecast_filtered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_fc_il(all_sales, res, month_to_correct=(6,'CNY', 11), thrsh=0.05):\n",
    "    \"\"\" This function is a post-processing step to the forecast, changing the forecast of some month to\n",
    "    correspond to past observed ratio\n",
    "    :param res: the data frame containing forecasts\n",
    "    :param month_to_correct: the months of forecast where we want to apply a post-process\n",
    "    :param thrsh: a threshold under which we do not perform any post-processing\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    sales = all_sales[all_sales.label=='il'].groupby(['calendar_yearmonth', 'label','country','brand','country_brand','tier','sub_brand'])['offtake'].sum().reset_index()\n",
    "    temp = res.copy()\n",
    "\n",
    "    years = list((res.date_to_predict // 100).unique())\n",
    "    tempil = temp[temp.label == 'il']\n",
    "    for y in years: \n",
    "        for m in month_to_correct:\n",
    "            for a in tempil.sub_brand.unique():\n",
    "                tempil = apply_forecast_correction(sales=sales, forecast=tempil, forecast_filtered=tempil, label='il',\n",
    "                                                   brand=a,year=y, month=m)\n",
    "    return tempil  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def correct_fc(all_sales,res, month_to_correct=(7, 'CNY', 11), thrsh=0.05):\n",
    "    \"\"\" This function is a post-processing step to the forecast, changing the forecast of some month to\n",
    "    correspond to past observed ratio\n",
    "    :param res: the data frame containing forecasts\n",
    "    :param month_to_correct: the months of forecast where we want to apply a post-process\n",
    "    :param thrsh: a threshold under which we do not perform any post-processing\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    sales = all_sales.groupby(['calendar_yearmonth'])['offtake'].sum().reset_index()\n",
    "    temp = res.copy()\n",
    "    years = list((res.date_to_predict.astype(int) // 100).unique())\n",
    "    tempdc = temp.copy()\n",
    "\n",
    "    for y in years:\n",
    "        for m in month_to_correct:\n",
    "            # Post-process for dc\n",
    "            tempdc = apply_forecast_correction(sales=sales, forecast=temp, forecast_filtered=tempdc, label='dc',\n",
    "                                               year=y, month=m, thrsh=thrsh)\n",
    "\n",
    "    return tempdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def fill_kpi_df(ytd_forecasts_kpis, df_, level, computed_on, scopes, months, horizons, first_month=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Fills the ytd KPIs dataframe\n",
    "    Inputs:\n",
    "      ytd_forecasts_kpis: dataframe to be filled\n",
    "      df_: the dataframe to be used for KPIs computation\n",
    "      computed_on: the scope where the KPIs are computed\n",
    "      scopes: list of scopes to take into account\n",
    "      months: list of months where to compute KPIs\n",
    "      horizons: list of horizons for KPIs computation. Can be an integer or \"R6M\"\n",
    "    Returns:\n",
    "      ytd_forecasts_kpis: the input datframe with new KPIs appended\n",
    "    \"\"\"\n",
    "    for scope in scopes:\n",
    "        for month in months:\n",
    "            for horizon in horizons: \n",
    "                if horizon == \"R6M\":\n",
    "                    r_bias = kpis.YTD_rolling_bias(6, 3, month, df_, scope=scope, agg_level=\"sku\", verbose=verbose)\n",
    "                    if r_bias is not None:\n",
    "                        r_bias = 100 * r_bias \n",
    "                    ytd_forecasts_kpis.loc[len(ytd_forecasts_kpis)] = [\"bias\", str(horizon), scope, level, computed_on, \"ytd\", month, r_bias]\n",
    "\n",
    "                    r_bias = kpis.rolling_bias(6, 3, month, df_, scope=scope, agg_level=\"sku\", verbose=verbose)\n",
    "                    if r_bias is not None:\n",
    "                        r_bias = 100 * r_bias\n",
    "                    ytd_forecasts_kpis.loc[len(ytd_forecasts_kpis)] = [\"bias\", str(horizon), scope, level, computed_on, \"month\", month, r_bias]\n",
    "\n",
    "                else:\n",
    "                    fa = kpis.YTD_FA(horizon, month, df_, scope=scope, agg_level=\"sku\", first_month=first_month, verbose=verbose)\n",
    "                    if fa is not None:\n",
    "                        fa = 100 * fa\n",
    "                    ytd_forecasts_kpis.loc[len(ytd_forecasts_kpis)] = [\"fa\", str(horizon), scope, level, computed_on, \"ytd\", month, fa]\n",
    "\n",
    "                    fa = kpis.FA(horizon, month, df_, scope=scope, agg_level=\"sku\", verbose=verbose)\n",
    "                    if fa is not None:\n",
    "                        fa = 100 * fa\n",
    "                    ytd_forecasts_kpis.loc[len(ytd_forecasts_kpis)] = [\"fa\", str(horizon), scope, level, computed_on, \"month\", month, fa]\n",
    "\n",
    "\n",
    "                    bias = kpis.YTD_bias(horizon, month, df_, scope=scope, agg_level=\"sku\", first_month=first_month, verbose=verbose)\n",
    "                    if bias is not None:\n",
    "                        bias = 100 * bias\n",
    "                    ytd_forecasts_kpis.loc[len(ytd_forecasts_kpis)] = [\"bias\", str(horizon), scope, level, computed_on, \"ytd\", month, bias]\n",
    "\n",
    "                    bias = kpis.bias(horizon, month, df_, scope=scope, agg_level=\"sku\", verbose=verbose)\n",
    "                    if bias is not None:\n",
    "                        bias = 100 * bias\n",
    "                    ytd_forecasts_kpis.loc[len(ytd_forecasts_kpis)] = [\"bias\", str(horizon), scope, level, computed_on, \"month\", month, bias]\n",
    "    return ytd_forecasts_kpis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preformat_table(sales_danone):\n",
    "    \"\"\" Preformat data table to feed into model\n",
    "    This function allows to precompute useful tables that will be used by the model\n",
    "    \"\"\"\n",
    "#         sales_danone = self.sales_danone.copy()\n",
    "    # 1. Formatting DI data\n",
    "    sales_danone_di = util.format_label_data(sales_danone, 'di')\n",
    "\n",
    "    # 2. Formatting EIB data\n",
    "    sales_danone_eib = util.format_label_data(sales_danone, 'eib')\n",
    "\n",
    "    # 3. Formatting IL data\n",
    "    sales_danone_il = util.format_label_data(sales_danone, 'il')\n",
    "\n",
    "    # 4. Merging data\n",
    "    all_sales = pd.concat([sales_danone_eib, sales_danone_di, sales_danone_il])\n",
    "\n",
    "    # 5. Selecting relevant columns\n",
    "    all_sales = all_sales[\n",
    "        ['calendar_yearmonth', 'country_brand', 'country', 'brand','tier','sub_brand','label', 'offtake', 'sellin', 'retailer_inv','sp_inv','sellout','price','upre','spre','mainstream','APT_si', 'C&G_si', 'Feihe_si', 'Friso_si', 'KC_si', 'NC_si', 'Weyth_si','brand_si',\n",
    "        'anp', 'tp', 'rebate', 'tp_rebate', 'sc_anp','sc_ts']]\n",
    "    return all_sales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_rolled_values(df: pd.DataFrame, date_col: str, granularity: List[str], value_cols: List[str], window_sizes: List[int]):\n",
    "    \"\"\" Calculate rolled features (mean, min, max) of selected columns over past k months.\n",
    "\n",
    "    :param df: Pandas dataframe containing\n",
    "    :param date_col: Name of date column\n",
    "    :param granularity: List of granularity columns\n",
    "    :param value_cols: List of columns for which we want to calculate rolled features\n",
    "    :param window_sizes: Size of rolling window\n",
    "    :return: Pandas dataframe containing rolled features\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.sort_values(date_col)\n",
    "\n",
    "    rolled_dfs = []\n",
    "    for window_size in window_sizes:\n",
    "        roll_df = df.set_index(date_col).groupby(granularity, as_index=True)[value_cols].rolling(window_size, min_periods=1)\n",
    "        roll_df_mean = roll_df.mean().fillna(0).reset_index()\n",
    "        roll_df_min = roll_df.min().fillna(0).reset_index()\n",
    "        roll_df_max = roll_df.max().fillna(0).reset_index()\n",
    "\n",
    "        roll_df_mean.columns = [str(col) + '_mean_%dM' % window_size if col not in [date_col] + granularity else col for col in roll_df_mean.columns]\n",
    "        roll_df_min.columns = [str(col) + '_min_%dM' % window_size if col not in [date_col] + granularity else col for col in roll_df_min.columns]\n",
    "        roll_df_max.columns = [str(col) + '_max_%dM' % window_size if col not in [date_col] + granularity else col for col in roll_df_max.columns]\n",
    "\n",
    "        res_df = pd.concat([roll_df_mean.set_index([date_col] + granularity),\n",
    "                                roll_df_min.set_index([date_col] + granularity),\n",
    "                                roll_df_max.set_index([date_col] + granularity)], axis=1)\n",
    "\n",
    "        rolled_dfs += [res_df]\n",
    "\n",
    "    concatenated_rolled_dfs = pd.concat(rolled_dfs, axis=1).reset_index()\n",
    "    df = pd.merge(left=df,\n",
    "                      right=concatenated_rolled_dfs,\n",
    "                      on=['date'] + granularity,\n",
    "                      how='left',\n",
    "                      validate='one_to_one',\n",
    "                      suffixes=(False, False))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_lagged_features(df: pd.DataFrame, groupby_cols: List[str], lags: List[int], cols_to_lag: List[str]) -> pd.DataFrame:\n",
    "    \"\"\" Add time-lagged values of selected columns\n",
    "\n",
    "    :param df: Pandas dataframe to which we want to add lagged values.\n",
    "    :param groupby_cols: Columns by which we want to group data before lagging\n",
    "    :param lags: List of temporal lags that we want to create values for\n",
    "    :param cols_to_lag: Columns which we want to lag\n",
    "    :return: Pandas dataframe with added lagged values.\n",
    "    \"\"\"\n",
    "    for col in cols_to_lag:\n",
    "        for lag in lags:\n",
    "            df[col + f'_lag{lag}'] = df.groupby(groupby_cols)[col].apply(pd.Series.shift, periods=lag).bfill()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and BackTesting the Model at Brand level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Format_label(df):\n",
    "    df_table = df.copy()\n",
    "    df_table.rename(columns = {'sub_brand_offtake_il':'offtake_il',\n",
    "                               'sub_brand_offtake_di':'offtake_di',\n",
    "                               'sub_brand_offtake_eib':'offtake_eib',\n",
    "                               'sub_brand_sellin_eib':'sellin_eib',\n",
    "                               'sub_brand_retailer_inv':'retailer_inv',\n",
    "                               'sub_brand_sellout':'sellout', \n",
    "                               'sub_brand_sp_inv':'sp_inv',\n",
    "                               'sub_brand_sellin_di':'sellin_di',\n",
    "                               'sub_brand_sellin_il':'sellin_il',\n",
    "                               'sub_brand_price':'price'}, inplace = True)\n",
    "    df_table = preformat_table(df_table)  \n",
    "    return df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_date_list(config):\n",
    "    \n",
    "    dwps = util.create_list_period(config[\"train_start\"], config[\"train_end\"], False)\n",
    "    dwp_test = util.create_list_period(config[\"backtest_start\"], config[\"backtest_end\"], False)\n",
    "    dwp, dtp = util.get_all_combination_date(dwps, 12) \n",
    "    \n",
    "    return dwp_test,dwp,dtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_feature(df, dwp, dtp):\n",
    "    \n",
    "    df = df[df.label=='il']\n",
    "\n",
    "    grouped_offtake2 =  df.groupby(['calendar_yearmonth','label','country','brand', 'tier','country_brand','sub_brand'])[\n",
    "                    'offtake'].sum().unstack('calendar_yearmonth').fillna(0)\n",
    "    df1 = features_amount_sales(grouped_offtake2, dwp, dtp, 'label_country_sub_brand_offtake_') \n",
    "    target = features_target(grouped_offtake2, dwp, dtp)\n",
    "#     grouped_offtake2 = df.groupby(['calendar_yearmonth', 'label','country','brand'])[\n",
    "#                     'offtake'].sum().unstack('calendar_yearmonth').fillna(0) \n",
    "#     df2 = features_amount_sales(grouped_offtake2, dwp, dtp, 'label_country_brand_offtake_') \n",
    "#     grouped_offtake2 = df.groupby(['calendar_yearmonth', 'label','brand','tier'])[\n",
    "#                     'offtake'].sum().unstack('calendar_yearmonth').fillna(0)\n",
    "#     df3 = features_amount_sales(grouped_offtake2, dwp, dtp, 'label_brand_tier_offtake_') \n",
    "#     grouped_offtake2 = df.groupby(['calendar_yearmonth', 'country','brand','tier'])[\n",
    "#                     'offtake'].sum().unstack('calendar_yearmonth').fillna(0)\n",
    "    # df4 = features_amount_sales(grouped_offtake2, dwp, dtp, 'country_brand_tier_offtake_') \n",
    "    # grouped_offtake2 = df.groupby(['calendar_yearmonth', 'country','brand'])[\n",
    "    #                 'offtake'].sum().unstack('calendar_yearmonth').fillna(0)\n",
    "    # df5 = features_amount_sales(grouped_offtake2, dwp, dtp, 'country_brand_offtake_') \n",
    "    # grouped_offtake2 = df.groupby(['calendar_yearmonth', 'country'])[\n",
    "    #                 'offtake'].sum().unstack('calendar_yearmonth').fillna(0)\n",
    "    # df6 = features_amount_sales(grouped_offtake2, dwp, dtp, 'country_offtake_')  \n",
    "\n",
    "    grouped_offtake2 = df.groupby(['calendar_yearmonth','label','country','brand', 'tier','country_brand','sub_brand'])[\n",
    "                    'sellin'].sum().unstack('calendar_yearmonth').fillna(0)\n",
    "    df7 = features_amount_sales(grouped_offtake2, dwp, dtp, 'label_country_brand_tier_sellin_') \n",
    "\n",
    "    # grouped_offtake2 = df.groupby(['calendar_yearmonth','country_brand'])[\n",
    "    #                 'sellin'].mean().unstack('calendar_yearmonth').fillna(0)\n",
    "    # df8 = features_amount_sales(grouped_offtake2, dwp, dtp, 'country_brand_sellin_', timelag=3) \n",
    "\n",
    "    df9 = create_seasonality_features(dtp)\n",
    "\n",
    "    grouped_offtake2 = df.groupby(['calendar_yearmonth','country_brand'])[\n",
    "                    'upre'].mean().unstack('calendar_yearmonth').fillna(0)\n",
    "    df10 = features_amount_sales(grouped_offtake2, dwp, dtp, 'upre_', timelag=3) \n",
    "\n",
    "    grouped_offtake2 = df.groupby(['calendar_yearmonth','country_brand'])[\n",
    "                    'spre'].mean().unstack('calendar_yearmonth').fillna(0)\n",
    "    df11 = features_amount_sales(grouped_offtake2, dwp, dtp, 'spre_', timelag=3) \n",
    "\n",
    "    grouped_offtake2 = df.groupby(['calendar_yearmonth','country_brand'])[\n",
    "                    'mainstream'].mean().unstack('calendar_yearmonth').fillna(0) \n",
    "    df12 = features_amount_sales(grouped_offtake2, dwp, dtp, 'mainstream_', timelag=3)\n",
    "\n",
    "    dfs = [df1,\n",
    "           df7,\n",
    "           df9,\n",
    "           df10,\n",
    "           df11,\n",
    "           df12,\n",
    "           target]\n",
    "\n",
    "    # Merging features\n",
    "    dffinal = reduce(lambda left, right: pd.merge(left, right, on=list(\n",
    "                {'date_when_predicting', 'date_to_predict', 'country', 'brand',  'label','tier',\n",
    "                 'country_brand','sub_brand'} & set(right.columns)), how='left'), dfs)\n",
    "\n",
    "    # Adding onehot encoding labels\n",
    "    one_hot_brand = pd.get_dummies(dffinal['country_brand']) \n",
    "    one_hot_tier = pd.get_dummies(dffinal['tier'])\n",
    "    # one_hot_label = pd.get_dummies(dffinal['label'])\n",
    "    dffinal = pd.concat([dffinal, \n",
    "                         one_hot_brand,\n",
    "                         one_hot_tier], axis=1)\n",
    "    \n",
    "    return dffinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling(df):\n",
    "\n",
    "    DEMAND_MODEL_GRANULARITY_COLS = ['sub_brand']\n",
    "    df_table_feature_rolling  = _add_rolled_values(df= df, \n",
    "        date_col='date',\n",
    "        granularity=DEMAND_MODEL_GRANULARITY_COLS,\n",
    "        value_cols=['sub_brand_offtake_il', 'sub_brand_sellin_il','sub_brand_offtake_di','price_tier_cat'], \n",
    "        window_sizes=[3, 6, 9, 12])\n",
    "\n",
    "    df_table_feature_rolling ['date_when_predicting'] = pd.to_datetime(df_table_feature_rolling ['date']).dt.year.astype(\n",
    "                str) + pd.to_datetime(df_table_feature_rolling ['date']).dt.month.astype(str).str.zfill(2)\n",
    "    df_table_feature_rolling ['date_when_predicting'] = df_table_feature_rolling ['date_when_predicting'].astype(int)\n",
    "\n",
    "    roll_features_sel = [c for c in df_table_feature_rolling.keys() if 'mean' in c] \n",
    "    \n",
    "    return df_table_feature_rolling,roll_features_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_category(config):\n",
    "    \n",
    "    category_path = config[\"project_folder_path\"] + '/' +\\\n",
    "                    config[\"data_folder_path\"] + '/' +\\\n",
    "                    config[\"input_category_forecast\"]\n",
    "    df_feature_addon = pd.read_csv(category_path)\n",
    "    \n",
    "    return df_feature_addon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_horizon(table_all_features):\n",
    "    \n",
    "    table_all_features['horizon'] = (pd.to_datetime(table_all_features.date_to_predict, format='%Y%m').dt.year -\n",
    "         pd.to_datetime(table_all_features.date_when_predicting, format='%Y%m').dt.year) * 12 + \\\n",
    "        (pd.to_datetime(table_all_features.date_to_predict, format='%Y%m').dt.month -\n",
    "         pd.to_datetime(table_all_features.date_when_predicting, format='%Y%m').dt.month) \n",
    "    \n",
    "    return table_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_table_with_category(table_all_features,df_feature_addon):\n",
    "    \n",
    "    table_all_features = table_all_features.merge(\n",
    "        df_feature_addon[config[\"features_cat_col\"] +\\\n",
    "                         config[\"features_cat_fsct_col\"] + \n",
    "                         ['label','country_brand','date_when_predicting','date_to_predict']], \n",
    "        left_on=['label','country_brand','date_when_predicting','date_to_predict'],\n",
    "        right_on =['label','country_brand','date_when_predicting','date_to_predict'])\n",
    "    \n",
    "    return table_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_table_with_rolling(table_all_features,df_table_feature_rolling,roll_features_sel):\n",
    "    \n",
    "    table_all_features = table_all_features.merge(\n",
    "        df_table_feature_rolling[['date_when_predicting','sub_brand']+roll_features_sel], \n",
    "        left_on =['date_when_predicting','sub_brand'],\n",
    "        right_on = ['date_when_predicting','sub_brand'])\n",
    "    \n",
    "    return table_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_single_run(config,dwp_test,table_all_features,df):\n",
    "    \n",
    "    resfinal = pd.DataFrame()\n",
    "    feature_importance_df_final = pd.DataFrame()\n",
    "\n",
    "    # Filter features to train the model\n",
    "    features = [x for x in table_all_features.keys() if (x not in config[\"features_int\"]) & \n",
    "                                                        (x in config[\"feature_test\"])]\n",
    "    \n",
    "    for datwep in dwp_test:\n",
    "        print(datwep) \n",
    "\n",
    "        res = pd.DataFrame()\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "        for h in range(1, config[\"horizon\"] + 1): \n",
    "            print(\"training model at horizon: \" + str(h))\n",
    "\n",
    "            subdata = table_all_features[(table_all_features.horizon == h) & (~table_all_features.target.isnull())\n",
    "                                                 & (table_all_features.date_to_predict <= datwep)]\n",
    "            if not config[\"FirstRun\"]:\n",
    "                features = list(feature_importance_df_sets[str(h)])+\\\n",
    "                            config[\"features_cat_col\"] +\\\n",
    "                            config[\"features_cat_fsct_col\"]\n",
    "\n",
    "            x_train = subdata[features].values\n",
    "            y_train = subdata.target \n",
    "            print(x_train.shape) \n",
    "\n",
    "            data_test = table_all_features[(table_all_features.date_when_predicting == datwep) &\n",
    "                                                   (table_all_features.horizon == h)].copy()\n",
    "\n",
    "            x_test = data_test[features].values\n",
    "\n",
    "            model = MLDCModel(\n",
    "                        model_name = config[\"model_config_RandomForestRegressor\"].model_name,\n",
    "                        model_params = config[\"model_config_RandomForestRegressor\"].model_params)\n",
    "\n",
    "            model.fit(x_train, y_train)\n",
    "            preds = model.predict(x_test)\n",
    "            preds = preds.clip(min=0)\n",
    "            data_test['horizon'] = h\n",
    "            data_test['prediction'] = preds \n",
    "            res = pd.concat([res, \n",
    "                             data_test[[\"label\",\n",
    "                                        \"date_to_predict\", \n",
    "                                        \"country\",\n",
    "                                        'brand',\n",
    "                                        \"tier\",\n",
    "                                        \"country_brand\",\n",
    "                                        \"sub_brand\",\n",
    "                                        \"prediction\", \n",
    "                                        \"horizon\"]]]) \n",
    "            feature_importance = dict(\n",
    "                        zip(features, zip(model.feature_importances_, [h] * len(model.feature_importances_))))\n",
    "            feature_importance = pd.DataFrame(feature_importance, index=['importance', 'horizon']).T\n",
    "            feature_importance_df = feature_importance_df.append(feature_importance.reset_index(),\n",
    "                                                                         ignore_index=True)\n",
    "\n",
    "        feature_importance_df['date_when_preidct'] = datwep\n",
    "        feature_importance = feature_importance_df\n",
    "        res_ = correct_fc_il(df_,res, month_to_correct=[6, 'CNY', 11], thrsh=0.05)\n",
    "        resfinal = pd.concat([resfinal, res_])   \n",
    "    return resfinal,feature_importance_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_feature_importance(config,feature_importance_df):\n",
    "    feature_importance_df_sets = dict() \n",
    "    for h in range(1,13):\n",
    "        feature_importance_df_sets[h] = feature_importance_df[feature_importance_df.horizon==h].\\\n",
    "        sort_values(['importance'], ascending = False) [:50]['index'].values\n",
    "    feature_importance_df_sets_ = pd.DataFrame(feature_importance_df_sets)\n",
    "    if config[\"FirstRun\"]:\n",
    "        feature_importance_df_sets_.to_csv(config[\"project_folder_path\"] +\\\n",
    "                                           '/' + config['temp_folder_path'] +\\\n",
    "                                           '/' + 'feature_importance_df_sets_0615_RF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_result_cal_kpi_score(resfinal,Test_name,df,config):\n",
    "    resfinal_brand = resfinal.groupby(['sub_brand','date_to_predict','label','horizon'])['prediction'].sum().reset_index()\n",
    "    df_brand = df.groupby(['sub_brand','label','calendar_yearmonth'])['offtake'].sum().reset_index()\n",
    "    res = pd.merge(left = resfinal_brand, right = df_brand[['calendar_yearmonth','sub_brand','label','offtake']],\n",
    "                   left_on = ['date_to_predict','sub_brand','label'],\n",
    "                   right_on = ['calendar_yearmonth','sub_brand','label'], \n",
    "                   how = 'left')   \n",
    "    res['forecasted_month'] = pd.to_datetime(res['date_to_predict'],format = '%Y%m')  \n",
    "    res['actual_month'] = res.apply (lambda x: x.forecasted_month-relativedelta(months = x.horizon), axis = 1)  \n",
    "    res['cycle_month'] = res.apply (lambda x: x.actual_month+relativedelta(months = 2), axis = 1)  \n",
    "\n",
    "    res['forecasted_month'] = res['forecasted_month'].apply(lambda x: x.strftime(\"%Y-%m\")) \n",
    "    res['actual_month'] = res['actual_month'].apply(lambda x: x.strftime(\"%Y-%m\")) \n",
    "    res['cycle_month'] = res['cycle_month'].apply(lambda x: x.strftime(\"%Y-%m\"))  \n",
    "    ytd_forecasts_kpis = pd.DataFrame(columns=[\"kpi_name\", \"horizon\", \"scope\", \"level\", \"computed_on\", \"kpi_type\", \"month\", \"value\"])\n",
    "    year = '2019'\n",
    "    months_2019 = ['%s-%.2i' %(year, month) for month in range(1,13,1)]\n",
    "\n",
    "    year = '2018'\n",
    "    months_2018 = ['%s-%.2i' %(year, month) for month in range(1,13,1)]\n",
    "\n",
    "    horizons = [1, 4, 'R6M']\n",
    "    first_month = None\n",
    "    verbose = False  \n",
    "    df_il = res.copy() \n",
    "    df_il['scope'] =df_il['label']\n",
    "    df_il = df_il[df_il.scope=='il'] \n",
    "    df_il.rename(columns = {'prediction':'forecast',\n",
    "                           'offtake':'actual'},inplace = True) \n",
    "    df_il[\"country_brand\"] = df_il[\"sub_brand\"].apply(lambda x: x.split(\"_\")[0]+'_'+x.split(\"_\")[1])\n",
    "    df_il[\"country\"] = df_il[\"country_brand\"].apply(lambda x: x.split(\"_\")[0])\n",
    "    df_il[\"brand\"] = df_il[\"country_brand\"].apply(lambda x: x.split(\"_\")[1])  \n",
    "    df_il.to_csv(config[\"project_folder_path\"] + '/' +\\\n",
    "                 config[\"result_folder_path\"] + '/' +\\\n",
    "                 'res' + Test_name + '.csv',index = False) \n",
    "#     df_il.loc[df_il.forecasted_month.isin(['2019-04','2019-05','2019-06']),'actual']=0\n",
    "#     df_il.loc[df_il.forecasted_month.isin(['2019-04','2019-05','2019-06']),'forecast']=0 \n",
    "    \n",
    "    # Compute YTD KPIs for different months\n",
    "    scopes = df_il.scope.unique()\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    computed_on: define on which scope to compute the KPIs:\n",
    "        * 'all_skus': compute the KPIs for all SKUs\n",
    "        * 'country': compute one KPI per country (name of country given in column \"computed_on\")\n",
    "        * 'brand': compute one KPI per brand (name of brand given in column \"computed_on\"). Note: one brand can be common to several countries\n",
    "        * 'country_brand': compute one KPI per brand per country (name of country & brand given in column \"computed_on\")\n",
    "    \"\"\"\n",
    "    computed_on_list = ['all_skus',\n",
    "                        'country',\n",
    "                        'brand',\n",
    "                        'country_brand',\n",
    "                        'sub_brand']\n",
    "\n",
    "    for computed_on in computed_on_list:\n",
    "        if computed_on == 'all_skus':\n",
    "            ytd_forecasts_kpis = fill_kpi_df(ytd_forecasts_kpis, df_il, computed_on, computed_on, scopes, months_2019, horizons, first_month=first_month, verbose=verbose)\n",
    "\n",
    "        else:\n",
    "            list_items = df_il[computed_on].unique()\n",
    "            for item in list_items: \n",
    "                df_ = df_il.copy()\n",
    "                df_ = df_.query(\"%s=='%s'\" %(computed_on, item))\n",
    "                ytd_forecasts_kpis = fill_kpi_df(ytd_forecasts_kpis, df_, computed_on, item, scopes, months_2019, horizons, first_month=first_month, verbose=verbose)   \n",
    "    ytd_forecasts_kpis_2019 = ytd_forecasts_kpis[(ytd_forecasts_kpis.month>='2019-01')&(ytd_forecasts_kpis.month<='2019-12')]\n",
    "    ytd_forecasts_kpis_2019.to_csv(config[\"project_folder_path\"] + '/' +\\\n",
    "                                   config[\"result_folder_path\"] + '/' +\\\n",
    "                                   'kpi_' +  Test_name + '.csv',index = False)         \n",
    "    df_kpi = ytd_forecasts_kpis_2019.copy() \n",
    "    df_kpi['version'] = Test_name\n",
    "    kpis_without_Q2 = kpi_stats.KPI_formalization(df_kpi,kpi_name = 'bias',skip_month = '2019-06', version = Test_name) \n",
    "    kpis_without_Q2.generate_KPI() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201801\n",
      "training model at horizon: 1\n",
      "(240, 60)\n",
      "training model at horizon: 2\n",
      "(230, 60)\n",
      "training model at horizon: 3\n",
      "(220, 60)\n",
      "training model at horizon: 4\n",
      "(210, 60)\n",
      "training model at horizon: 5\n",
      "(200, 60)\n",
      "training model at horizon: 6\n",
      "(190, 60)\n",
      "training model at horizon: 7\n",
      "(180, 60)\n",
      "training model at horizon: 8\n",
      "(170, 60)\n",
      "training model at horizon: 9\n",
      "(160, 60)\n",
      "training model at horizon: 10\n",
      "(150, 60)\n",
      "training model at horizon: 11\n",
      "(140, 60)\n",
      "training model at horizon: 12\n",
      "(130, 60)\n",
      "201802\n",
      "training model at horizon: 1\n",
      "(250, 60)\n",
      "training model at horizon: 2\n",
      "(240, 60)\n",
      "training model at horizon: 3\n",
      "(230, 60)\n",
      "training model at horizon: 4\n",
      "(220, 60)\n",
      "training model at horizon: 5\n",
      "(210, 60)\n",
      "training model at horizon: 6\n",
      "(200, 60)\n",
      "training model at horizon: 7\n",
      "(190, 60)\n",
      "training model at horizon: 8\n",
      "(180, 60)\n",
      "training model at horizon: 9\n",
      "(170, 60)\n",
      "training model at horizon: 10\n",
      "(160, 60)\n",
      "training model at horizon: 11\n",
      "(150, 60)\n",
      "training model at horizon: 12\n",
      "(140, 60)\n",
      "201803\n",
      "training model at horizon: 1\n",
      "(260, 60)\n",
      "training model at horizon: 2\n",
      "(250, 60)\n",
      "training model at horizon: 3\n",
      "(240, 60)\n",
      "training model at horizon: 4\n",
      "(230, 60)\n",
      "training model at horizon: 5\n",
      "(220, 60)\n",
      "training model at horizon: 6\n",
      "(210, 60)\n",
      "training model at horizon: 7\n",
      "(200, 60)\n",
      "training model at horizon: 8\n",
      "(190, 60)\n",
      "training model at horizon: 9\n",
      "(180, 60)\n",
      "training model at horizon: 10\n",
      "(170, 60)\n",
      "training model at horizon: 11\n",
      "(160, 60)\n",
      "training model at horizon: 12\n",
      "(150, 60)\n",
      "201804\n",
      "training model at horizon: 1\n",
      "(270, 60)\n",
      "training model at horizon: 2\n",
      "(260, 60)\n",
      "training model at horizon: 3\n",
      "(250, 60)\n",
      "training model at horizon: 4\n",
      "(240, 60)\n",
      "training model at horizon: 5\n",
      "(230, 60)\n",
      "training model at horizon: 6\n",
      "(220, 60)\n",
      "training model at horizon: 7\n",
      "(210, 60)\n",
      "training model at horizon: 8\n",
      "(200, 60)\n",
      "training model at horizon: 9\n",
      "(190, 60)\n",
      "training model at horizon: 10\n",
      "(180, 60)\n",
      "training model at horizon: 11\n",
      "(170, 60)\n",
      "training model at horizon: 12\n",
      "(160, 60)\n",
      "201805\n",
      "training model at horizon: 1\n",
      "(280, 60)\n",
      "training model at horizon: 2\n",
      "(270, 60)\n",
      "training model at horizon: 3\n",
      "(260, 60)\n",
      "training model at horizon: 4\n",
      "(250, 60)\n",
      "training model at horizon: 5\n",
      "(240, 60)\n",
      "training model at horizon: 6\n",
      "(230, 60)\n",
      "training model at horizon: 7\n",
      "(220, 60)\n",
      "training model at horizon: 8\n",
      "(210, 60)\n",
      "training model at horizon: 9\n",
      "(200, 60)\n",
      "training model at horizon: 10\n",
      "(190, 60)\n",
      "training model at horizon: 11\n",
      "(180, 60)\n",
      "training model at horizon: 12\n",
      "(170, 60)\n",
      "201806\n",
      "training model at horizon: 1\n",
      "(290, 60)\n",
      "training model at horizon: 2\n",
      "(280, 60)\n",
      "training model at horizon: 3\n",
      "(270, 60)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 1.Load data\n",
    "    df = Load_raw_master(config)                                          ## 1.1 Load raw_master\n",
    "    df_feature_addon = Load_category(config)                              ## 1.2 Load category and category forecast data\n",
    "    if not config[\"FirstRun\"]: \n",
    "        feature_importance_df_sets = Load_50_feature(config)              ## 1.3 Load rank firt 50 features\n",
    "    \n",
    "    # 2.Feature Engineering\n",
    "    df_table_feature_rolling,roll_features_sel = get_rolling(df)          ## 2.1 Calculate rolling fetures\n",
    "    dwp_test,dwp,dtp = Create_date_list(config)                           ## 2.2 Calculate date list\n",
    "    \n",
    "    df_ = Format_label(df)                                                ## 2.3 Format tables\n",
    "    table_with_features = Create_feature(df_, dwp, dtp)                   ## 2.4 Create features\n",
    "    table_with_features = Calculate_horizon(table_with_features)          ## 2.5 Calculate horizons \n",
    "    table_all_features = Merge_table_with_rolling(table_with_features,\n",
    "                                                  df_table_feature_rolling,\n",
    "                                                  roll_features_sel)       ## 2.6 Merge category features\n",
    "    table_all_features = Merge_table_with_category(table_all_features,\n",
    "                                                   df_feature_addon)       ## 2.7 Merge category features\n",
    "    # 3.Train Model\n",
    "    resfinal,feature_importance_df = model_single_run(\n",
    "        config,dwp_test,table_all_features,df)                             ## 3.1 Train model and output the forecast\n",
    "    output_feature_importance(config,feature_importance_df)                ## 3.2 Output feature importance\n",
    "    format_result_cal_kpi_score(resfinal,config[\"version_name\"],df_,config)## 3.3 Output KPI score"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "253px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
